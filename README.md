# æƒ…æ„Ÿåˆ†æç³»ç»Ÿ (Sentiment Analysis System)

ä¸€ä¸ªåŸºäº PyTorch å’Œ BERT çš„ Twitter æ–‡æœ¬æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œå®ç°äº† **98% éªŒè¯å‡†ç¡®ç‡**ï¼Œæ”¯æŒ CPU/GPU è®­ç»ƒã€‚

## ğŸ“Š é¡¹ç›®æ¦‚è§ˆ

### æ•°æ®é›†
- **è®­ç»ƒé›†**: 72,142 æ ·æœ¬ (twitter_training_cleaned.csv)
- **éªŒè¯é›†**: 1,000 æ ·æœ¬ (twitter_validation_cleaned.csv)
- **åˆ†ç±»ç±»åˆ«**: 4 ç±»ï¼ˆIrrelevant, Negative, Neutral, Positiveï¼‰

### æ¨¡å‹æ€§èƒ½
| æ¨¡å‹ | å‡†ç¡®ç‡ | å‚æ•°é‡ | å¤‡æ³¨ |
|------|--------|--------|------|
| **BERT-base (æœ€ç»ˆ)** | **98.0%** âœ… | 43.6M (å¯è®­ç»ƒ) | å†»ç»“å‰6å±‚ï¼Œå¾®è°ƒå6å±‚ |
| SVM | 96.2% | - | åŸºçº¿æ¨¡å‹ |
| GRU | ~96% | - | è‡ªå®šä¹‰ GRU æ¶æ„ |
| Random Forest | 95.9% | - | åŸºçº¿æ¨¡å‹ |
| KNN | 93.6% | - | åŸºçº¿æ¨¡å‹ |
| XGBoost | 93.2% | - | åŸºçº¿æ¨¡å‹ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚
```bash
Python >= 3.8
PyTorch >= 1.9
transformers >= 4.20
pandas, numpy, scikit-learn, matplotlib, seaborn
```

### ç¯å¢ƒé…ç½®

#### é€‰é¡¹ 1: æœ¬åœ° CPU ç¯å¢ƒ
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install transformers pandas numpy scikit-learn matplotlib seaborn
```

#### é€‰é¡¹ 2: GPU ç¯å¢ƒï¼ˆCUDA 11.8ï¼‰
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers pandas numpy scikit-learn matplotlib seaborn
```

#### é€‰é¡¹ 3: ä½¿ç”¨ conda
```bash
conda create -n sentiment-analysis python=3.10
conda activate sentiment-analysis
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
pip install transformers pandas scikit-learn matplotlib seaborn
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
SentimentAnalysisSystem/
â”œâ”€â”€ README.md                           # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ LICENSE                             # å¼€æºåè®®
â”œâ”€â”€ .gitignore                          # Git é…ç½®
â”œâ”€â”€ down_models.py                      # ä¸‹è½½æ¨¡å‹
|
â”œâ”€â”€ ã€BERT æ¨¡å‹ - æœ€ç»ˆç‰ˆæœ¬ã€‘
â”œâ”€â”€ BERT_main_final.py                  # â­ BERT è®­ç»ƒä¸»ç¨‹åºï¼ˆ98%å‡†ç¡®ç‡ï¼‰
â”œâ”€â”€ BERT_model_final.py                 # â­ BERT æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒå‡½æ•°
â”œâ”€â”€ BERT_config.py                      # BERT é…ç½®ï¼ˆæœ¬åœ°/åœ¨çº¿æ¨¡å¼ï¼‰
â”‚
â”œâ”€â”€ ã€RNN åŸºçº¿æ¨¡å‹ã€‘
â”œâ”€â”€ RNN_main.py                         # RNN è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ RNN_model.py                        # RNN æ¨¡å‹å®šä¹‰
â”‚
â”œâ”€â”€ ã€æ•°æ®å¤„ç†ã€‘
â”œâ”€â”€ text_process.py                     # æ–‡æœ¬é¢„å¤„ç†å·¥å…·
â”œâ”€â”€ data_cleaning.ipynb                 # æ•°æ®æ¸…æ´—è¿‡ç¨‹è®°å½•
â”œâ”€â”€ baseline_model.ipynb                # åŸºçº¿æ¨¡å‹å®éªŒè®°å½•
â”‚
â”œâ”€â”€ ã€æ•°æ®é›†ã€‘
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ twitter_training_cleaned.csv    # â­ æ¸…æ´—åçš„è®­ç»ƒé›†ï¼ˆ72,142æ¡ï¼‰
â”‚   â”œâ”€â”€ twitter_validation_cleaned.csv  # â­ æ¸…æ´—åçš„éªŒè¯é›†ï¼ˆ1,000æ¡ï¼‰
â”‚   â”œâ”€â”€ twitter_training.csv            # åŸå§‹è®­ç»ƒé›†
â”‚   â””â”€â”€ twitter_validation.csv          # åŸå§‹éªŒè¯é›†
â”‚
â”œâ”€â”€ ã€é¢„è®­ç»ƒæ¨¡å‹ã€‘
â”œâ”€â”€ models/
â”‚   â””â”€â”€ bert-base-uncased/              # â­ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œdown_models.pyï¼‰
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â””â”€â”€ vocab.txt
â”‚
â”œâ”€â”€ ã€è®­ç»ƒè¾“å‡ºã€‘ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â”œâ”€â”€ best_model_simple.pth               # â­ è®­ç»ƒå¥½çš„æœ€ä½³æ¨¡å‹ï¼ˆ98%å‡†ç¡®ç‡ï¼‰
â”œâ”€â”€ training_log.txt                    # å®Œæ•´è®­ç»ƒæ—¥å¿—
â”‚
â”œâ”€â”€ ã€é¡¹ç›®ç»´æŠ¤ã€‘
â”œâ”€â”€ cleanup_final.py                    # é¡¹ç›®æ¸…ç†è„šæœ¬
â”œâ”€â”€ PROJECT_CLEANUP_FINAL.md            # æ¸…ç†æŒ‡å—
â””â”€â”€ archive_debug_files/                # è°ƒè¯•æ–‡ä»¶å½’æ¡£ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ test_overfitting*.py            # è¿‡æ‹Ÿåˆæµ‹è¯•
    â”œâ”€â”€ diagnose_*.py                   # è¯Šæ–­è„šæœ¬
    â”œâ”€â”€ check_*.py                      # æ£€æŸ¥å·¥å…·
    â””â”€â”€ BERT_*_old.py                   # æ—§ç‰ˆæœ¬è®­ç»ƒä»£ç 
```

## ğŸ“‹ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹ï¼šBERT æ¨¡å‹è®­ç»ƒ

**å‰ç½®æ¡ä»¶**ï¼š
- å·²å‡†å¤‡å¥½æ•°æ®é›†ï¼š`dataset/twitter_training_cleaned.csv` å’Œ `dataset/twitter_validation_cleaned.csv`
- å·²è¿è¡Œdown_models.pyï¼Œä¸‹è½½ BERT æ¨¡å‹åˆ° `models/bert-base-uncased/`

**ä¸€é”®è®­ç»ƒ**ï¼š
```bash
python BERT_main_final.py
```

**è®­ç»ƒé…ç½®ï¼ˆå·²ä¼˜åŒ–åˆ°æœ€ä½³æ€§èƒ½ï¼‰**ï¼š
- å­¦ä¹ ç‡ï¼š`1e-5`
- Batch Sizeï¼š`8`
- è®­ç»ƒè½®æ•°ï¼š`50`ï¼ˆé€šå¸¸15-20è½®å³å¯æ”¶æ•›ï¼‰
- å†»ç»“ç­–ç•¥ï¼š`half`ï¼ˆå†»ç»“å‰6å±‚ï¼Œè®­ç»ƒå6å±‚ï¼‰
- Dropoutï¼š`0.1`
- æ¢¯åº¦è£å‰ªï¼š`1.0`

**é¢„æœŸç»“æœ**ï¼š
- Epoch 1ï¼šéªŒè¯å‡†ç¡®ç‡ ~69%
- Epoch 5-10ï¼šéªŒè¯å‡†ç¡®ç‡ ~85-90%
- Epoch 15-20ï¼šéªŒè¯å‡†ç¡®ç‡ ~95-98%

**è®­ç»ƒå®Œæˆå**ï¼š
- æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ï¼š`best_model_simple.pth`
- è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ï¼š`training_log.txt`

### RNNï¼ˆå®é™…ä¸Šæ˜¯GRUï¼‰ åŸºçº¿æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

```bash
python RNN_main.py
```

### ä¸‹è½½ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰

å¦‚æœ `models/bert-base-uncased/` æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆä¸‹è½½æ¨¡å‹ï¼š

```python
# æ–¹æ³•1ï¼šä½¿ç”¨ transformers åº“è‡ªåŠ¨ä¸‹è½½ï¼ˆéœ€è¦ç½‘ç»œï¼‰
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('bert-base-uncased', cache_dir='./models')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir='./models')

# æ–¹æ³•2ï¼šè¿è¡Œdown_models.pyï¼ˆæ¨èï¼Œéœ€è¦ç½‘ç»œï¼‰

```



## ğŸ”§ ç¯å¢ƒé…ç½®

### å‰ç½®è¦æ±‚
```bash
Python >= 3.8
PyTorch >= 1.9
transformers >= 4.20
pandas, numpy, scikit-learn
```

### å®‰è£…ä¾èµ–

#### é€‰é¡¹ 1: GPU ç¯å¢ƒï¼ˆæ¨èï¼Œç”¨äºè®­ç»ƒï¼‰
```bash
# å®‰è£… PyTorch with CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers pandas numpy scikit-learn
```

#### é€‰é¡¹ 2: CPU ç¯å¢ƒï¼ˆç”¨äºæ¨ç†ï¼‰
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install transformers pandas numpy scikit-learn
```

#### é€‰é¡¹ 3: ä½¿ç”¨ conda
```bash
conda create -n sentiment python=3.10
conda activate sentiment
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
pip install transformers pandas scikit-learn
```

### æ­¥éª¤ 2: ä¸Šä¼ åˆ°æœåŠ¡å™¨

å°†ä»¥ä¸‹å†…å®¹ä¸Šä¼ åˆ°äº‘æœåŠ¡å™¨ï¼š
```
SentimentAnalysisSystem/
â”œâ”€â”€ BERT_main.py
â”œâ”€â”€ BERT_model.py
â”œâ”€â”€ BERT_config.py
â”œâ”€â”€ text_process.py
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ twitter_training_cleaned.csv
â”‚   â””â”€â”€ twitter_validation_cleaned.csv
â””â”€â”€ models/                    â† å…³é”®ï¼šåŒ…å«ä¸‹è½½çš„æ¨¡å‹
    â””â”€â”€ bert-base-uncased/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â”œâ”€â”€ tokenizer.json
        â””â”€â”€ ...
```

### æ­¥éª¤ 3: æœåŠ¡å™¨é…ç½®

ç™»å½•åˆ°äº‘æœåŠ¡å™¨åï¼Œç¼–è¾‘ `BERT_config.py`ï¼š

```python
# BERT_config.py

# é€‰æ‹©æ¨¡å¼
MODE = 'offline'  # âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆæ¨èï¼‰
# MODE = 'online'   # å¦‚æœæœåŠ¡å™¨èƒ½è®¿é—® HuggingFaceï¼Œå¯ä»¥æ”¹ä¸ºåœ¨çº¿

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODELS_DIR = './models'  # ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
LOCAL_MODEL_NAME = 'bert-base-uncased'
```

### æ­¥éª¤ 4: è®­ç»ƒæ¨¡å‹

åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œè®­ç»ƒï¼š

#### 4.1 å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯ä»£ç å¯ç”¨ï¼‰
```bash
# ä¿®æ”¹ BERT_main.py ä¸­çš„ NUM_EPOCHS = 1 è¿›è¡Œå¿«é€Ÿæµ‹è¯•
python BERT_main.py
```

#### 4.2 å®Œæ•´è®­ç»ƒ
```bash
# æ¢å¤ BERT_main.py ä¸­çš„ NUM_EPOCHS = 5
python BERT_main.py
```

è®­ç»ƒè¾“å‡ºç¤ºä¾‹ï¼š
```
============================================================
BERT æƒ…æ„Ÿåˆ†ææ¨¡å‹ - Kaggle GPU ä¼˜åŒ–ç‰ˆæœ¬
============================================================

============================================================
BERT æ¨¡å‹é…ç½®
============================================================
æ¨¡å¼: offline
æœ¬åœ°æ¨¡å‹ç›®å½•: /root/Workspace/SentimentAnalysisSystem/models/bert-base-uncased
âœ… æ¨¡å‹å·²å­˜åœ¨ (6 ä¸ªæ–‡ä»¶, 417.92 MB)
============================================================

æ­¥éª¤ 1: åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
============================================================
è®­ç»ƒé›†å¤§å°: 72142
éªŒè¯é›†å¤§å°: 1000
æ£€æµ‹åˆ° 4 ä¸ªç±»åˆ«:
æ ‡ç­¾æ˜ å°„: {0: 'Irrelevant', 1: 'Negative', 2: 'Neutral', 3: 'Positive'}

æ­¥éª¤ 2: åˆå§‹åŒ–åˆ†è¯å™¨
============================================================
åŠ è½½åˆ†è¯å™¨: /root/Workspace/SentimentAnalysisSystem/models/bert-base-uncased
ä»æœ¬åœ°åŠ è½½åˆ†è¯å™¨...

...

Epoch 1/5
Epoch [1/5] Batch [50/564] Loss: 0.5621
Epoch [1/5] Batch [100/564] Loss: 0.4532
...

è®­ç»ƒæŸå¤±: 0.2145 | è®­ç»ƒå‡†ç¡®ç‡: 0.9234
éªŒè¯æŸå¤±: 0.2856 | éªŒè¯å‡†ç¡®ç‡: 0.9156
```

### æ­¥éª¤ 5: æŸ¥çœ‹ç»“æœ

è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
- `bert_training_history.png` - è®­ç»ƒæ›²çº¿
- `bert_confusion_matrix.png` - æ··æ·†çŸ©é˜µ
- `bert_model_final.pt` - æœ€ç»ˆæ¨¡å‹
- æ§åˆ¶å°è¾“å‡ºè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š

## ğŸ¯ å…³é”®æŠ€æœ¯è¦ç‚¹

### BERT æ¨¡å‹ä¼˜åŒ–ç­–ç•¥

**æœ€ç»ˆæˆåŠŸé…ç½®**ï¼ˆ98%å‡†ç¡®ç‡ï¼‰ï¼š
```python
BATCH_SIZE = 8              # å°æ‰¹æ¬¡ï¼Œæ›´é¢‘ç¹çš„æƒé‡æ›´æ–°
LEARNING_RATE = 1e-5        # ä½å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒ
FREEZE_STRATEGY = 'half'    # å†»ç»“å‰6å±‚ï¼Œåªè®­ç»ƒå6å±‚
DROPOUT = 0.1               # ä½dropoutï¼Œå……åˆ†å­¦ä¹ 
MAX_GRAD_NORM = 1.0         # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
NUM_EPOCHS = 50             # å……è¶³çš„è®­ç»ƒè½®æ•°
```

### è®­ç»ƒå¤±è´¥åŸå› æ€»ç»“ï¼ˆä¾›å‚è€ƒï¼‰

ç»è¿‡å¤šæ¬¡å®éªŒï¼Œå‘ç°ä»¥ä¸‹é…ç½®ä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼ˆå‡†ç¡®ç‡åœç•™åœ¨25-30%ï¼‰ï¼š

âŒ **é”™è¯¯é…ç½®**ï¼š
- æ¢¯åº¦ç´¯ç§¯æ­¥æ•°è¿‡å¤§ï¼ˆ>1ï¼‰ï¼šå¯¼è‡´æœ‰æ•ˆbatch sizeè¿‡å¤§ï¼ˆ64+ï¼‰ï¼Œæ›´æ–°é¢‘ç‡å¤ªä½
- å­¦ä¹ ç‡è¿‡é«˜ï¼ˆ>2e-5ï¼‰ï¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
- Dropoutè¿‡é«˜ï¼ˆ>0.3ï¼‰ï¼šè®­ç»ƒæ—¶ä¸¢å¼ƒè¿‡å¤šä¿¡æ¯
- å®Œå…¨è§£å†»BERTï¼š110Må‚æ•°åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆ

âœ… **æˆåŠŸè¦ç´ **ï¼š
1. **å°batch sizeï¼ˆ8ï¼‰+ æ— æ¢¯åº¦ç´¯ç§¯** â†’ é¢‘ç¹æ›´æ–°æƒé‡
2. **ä½å­¦ä¹ ç‡ï¼ˆ1e-5ï¼‰** â†’ ç¨³å®šçš„æ¢¯åº¦ä¸‹é™
3. **éƒ¨åˆ†å†»ç»“ï¼ˆhalfï¼‰** â†’ å‡å°‘å¯è®­ç»ƒå‚æ•°åˆ°43.6M
4. **ä½dropoutï¼ˆ0.1ï¼‰** â†’ ä¿ç•™æ›´å¤šè®­ç»ƒä¿¡æ¯
5. **æ¢¯åº¦è£å‰ªï¼ˆ1.0ï¼‰** â†’ é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### æ€§èƒ½å¯¹æ¯”

| é…ç½® | Epoch 1 å‡†ç¡®ç‡ | æœ€ç»ˆå‡†ç¡®ç‡ | æ”¶æ•›è½®æ•° |
|------|----------------|-----------|---------|
| å¤±è´¥é…ç½® | ~30% | ~30% | ä¸æ”¶æ•› |
| æˆåŠŸé…ç½® | **69%** | **98%** | 15-20è½® |

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š
- **Epoch 1 éªŒè¯å‡†ç¡®ç‡**ï¼šåº”è¯¥åœ¨ 60-70% ä¹‹é—´
- **Loss ä¸‹é™è¶‹åŠ¿**ï¼šåº”è¯¥å¹³ç¨³ä¸‹é™ï¼Œä¸åº”è¯¥éœ‡è¡æˆ–ä¸Šå‡
- **è®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡å·®è·**ï¼šå·®è·è¿‡å¤§è¯´æ˜è¿‡æ‹Ÿåˆ

### æ—©åœç­–ç•¥

ä»£ç å†…ç½®æ—©åœæœºåˆ¶ï¼š
```python
patience = 10  # è¿ç»­10è½®æ— æå‡åˆ™åœæ­¢
```

### æ¨¡å‹æ¨ç†

åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼š

```python
import torch
from BERT_model_final import BertSentimentClassifier
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹
model = BertSentimentClassifier(
    model_name='models/bert-base-uncased',
    num_classes=4,
    dropout=0.1,
    freeze_strategy='half'
)
model.load_state_dict(torch.load('best_model_simple.pth'))
model.eval()

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained('models/bert-base-uncased', local_files_only=True)

# é¢„æµ‹
text = "I love this product!"
encoding = tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')

with torch.no_grad():
    output = model(encoding['input_ids'], encoding['attention_mask'])
    prediction = torch.argmax(output, dim=1).item()

label_map = {0: 'Irrelevant', 1: 'Negative', 2: 'Neutral', 3: 'Positive'}
print(f"é¢„æµ‹ç»“æœ: {label_map[prediction]}")
```

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: è®­ç»ƒå‡†ç¡®ç‡åœåœ¨ 25-30%
**åŸå› **: æ¢¯åº¦ç´¯ç§¯è¿‡å¤§æˆ–å­¦ä¹ ç‡ä¸å½“  
**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ `BERT_main_final.py` çš„é»˜è®¤é…ç½®

### é—®é¢˜ 2: æ˜¾å­˜ä¸è¶³
```
CUDA out of memory
```
**è§£å†³æ–¹æ¡ˆ**:
- å‡å°‘ `BATCH_SIZE` (8 â†’ 4)
- å‡å°‘ `MAX_LENGTH` (128 â†’ 96)

### é—®é¢˜ 3: æ¨¡å‹åŠ è½½å¤±è´¥
```
FileNotFoundError: models/bert-base-uncased
```
**è§£å†³æ–¹æ¡ˆ**: ä¸‹è½½ BERT æ¨¡å‹åˆ° `models/` æ–‡ä»¶å¤¹

## ğŸ“š æ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š
- `cleaned_text`: æ¸…æ´—åçš„æ–‡æœ¬
- `attitude`: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆIrrelevant/Negative/Neutral/Positive æˆ– 0/1/2/3ï¼‰

ç¤ºä¾‹ï¼š
```csv
cleaned_text,attitude
im getting on borderlands and i will murder you all,Positive
this movie is great,Positive
i hate this,Negative
it is okay,Neutral
not relevant,Irrelevant
```

## ğŸ“Š é¡¹ç›®æˆæœ

### æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
- **éªŒè¯å‡†ç¡®ç‡**: 98.0%
- **è®­ç»ƒå‡†ç¡®ç‡**: 99.5%
- **è®­ç»ƒæ—¶é•¿**: ~2å°æ—¶ï¼ˆRTX 4090ï¼Œ20 epochsï¼‰
- **æ”¶æ•›é€Ÿåº¦**: 15-20 epochs

### ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”
- è¶…è¶Š SVM (96.2%) **+1.8%**
- è¶…è¶Š GRU (~96%) **+2.0%**
- è¶…è¶Š Random Forest (95.9%) **+2.1%**

## ğŸ“š å‚è€ƒèµ„æº

- [BERT è®ºæ–‡](https://arxiv.org/abs/1810.04805)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)
- [PyTorch æ–‡æ¡£](https://pytorch.org/docs/)

## ğŸ“ é¡¹ç›®ç»´æŠ¤

### æ¸…ç†è°ƒè¯•æ–‡ä»¶

é¡¹ç›®åŒ…å«å¤§é‡è°ƒè¯•å’Œæµ‹è¯•æ–‡ä»¶ï¼Œå·²ç§»è‡³ `archive_debug_files/`ï¼š

```bash
# æŸ¥çœ‹å½’æ¡£å†…å®¹
ls archive_debug_files/

# å¦‚éœ€æ¢å¤æŸä¸ªæ–‡ä»¶
cp archive_debug_files/test_overfitting_v2.py .
```

### æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ç”¨é€” | æ˜¯å¦å¿…éœ€ |
|------|------|---------|
| `BERT_main_final.py` | è®­ç»ƒè„šæœ¬ | âœ… å¿…éœ€ |
| `BERT_model_final.py` | æ¨¡å‹å®šä¹‰ | âœ… å¿…éœ€ |
| `best_model_simple.pth` | è®­ç»ƒå¥½çš„æ¨¡å‹ | âœ… å¿…éœ€ï¼ˆæ¨ç†ç”¨ï¼‰|
| `models/bert-base-uncased/` | é¢„è®­ç»ƒæ¨¡å‹ | âœ… å¿…éœ€ |
| `dataset/*.csv` | æ•°æ®é›† | âœ… å¿…éœ€ï¼ˆè®­ç»ƒç”¨ï¼‰|
| `archive_debug_files/` | è°ƒè¯•æ–‡ä»¶ | âŒ å¯é€‰ |

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- HuggingFace æä¾›é¢„è®­ç»ƒ BERT æ¨¡å‹
- PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
- Kaggle æä¾›æ•°æ®é›†

---

**é¡¹ç›®çŠ¶æ€**: âœ… å·²å®Œæˆ (2025-12-06)  
**æœ€ç»ˆå‡†ç¡®ç‡**: 98.0%  
**ä½œè€…**: F010110

**æœ€åæ›´æ–°**: 2025-12-06  
**ç‰ˆæœ¬**: 3.0 (BERT + GRU)

