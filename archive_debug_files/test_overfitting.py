"""
è¿‡æ‹Ÿåˆæµ‹è¯•è„šæœ¬
ç›®çš„ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦å…·å¤‡å­¦ä¹ èƒ½åŠ›
æ–¹æ³•ï¼šä½¿ç”¨æå°æ•°æ®é›†ï¼ˆ100æ¡ï¼‰ï¼Œè®­ç»ƒå¤šè½®ï¼Œè§‚å¯Ÿæ˜¯å¦èƒ½è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒå‡†ç¡®ç‡æ¥è¿‘100%ï¼‰

å¦‚æœæ¨¡å‹èƒ½è¿‡æ‹Ÿåˆå°æ•°æ®é›† â†’ æ¨¡å‹æ¶æ„OKï¼Œé—®é¢˜åœ¨äºå¤§æ•°æ®é›†è®­ç»ƒç­–ç•¥
å¦‚æœæ¨¡å‹æ— æ³•è¿‡æ‹Ÿåˆ â†’ æ¨¡å‹æ¶æ„/å®ç°æœ‰é—®é¢˜
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, get_linear_schedule_with_warmup
import pandas as pd
import numpy as np
from tqdm import tqdm
import sys
import os

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append(os.path.dirname(__file__))
from BERT_model_final import BertSentimentClassifier

# ==================== é…ç½®å‚æ•° ====================
MODEL_NAME = 'models/bert-base-uncased'  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
TRAIN_DATA_PATH = 'dataset/twitter_training_cleaned.csv'

# å°æ•°æ®é›†é…ç½®
TINY_DATASET_SIZE = 100  # åªç”¨100æ¡æ•°æ®
BATCH_SIZE = 8           # å°æ‰¹æ¬¡
LEARNING_RATE = 5e-5     # è¾ƒé«˜å­¦ä¹ ç‡ï¼ˆä¸ºäº†å¿«é€Ÿè¿‡æ‹Ÿåˆï¼‰
NUM_EPOCHS = 50          # å¤šè½®è®­ç»ƒ
MAX_LENGTH = 128

# å…¶ä»–å‚æ•°
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
NUM_CLASSES = 4
DROPOUT = 0.1  # é™ä½ dropoutï¼ˆå‡å°‘æ­£åˆ™åŒ–ï¼Œæ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰

# ==================== æ•°æ®é›† ====================
class SentimentDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.texts = dataframe['cleaned_text'].values
        self.labels = dataframe['attitude_encoded'].values
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy

def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy

# ==================== ä¸»å‡½æ•° ====================
def main():
    print("="*70)
    print("BERT è¿‡æ‹Ÿåˆæµ‹è¯• - éªŒè¯æ¨¡å‹å­¦ä¹ èƒ½åŠ›")
    print("="*70)
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  æ•°æ®é›†å¤§å°: {TINY_DATASET_SIZE} æ¡")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"  è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"  Dropout: {DROPOUT}")
    print(f"  è®¾å¤‡: {DEVICE}")
    print(f"\né¢„æœŸç»“æœ:")
    print(f"  âœ“ å¦‚æœæ¨¡å‹èƒ½å­¦ä¹ ï¼šè®­ç»ƒå‡†ç¡®ç‡åº”è¯¥åœ¨ 10-20 è½®å†…è¾¾åˆ° 90%+")
    print(f"  âœ— å¦‚æœæ¨¡å‹æ— æ³•å­¦ä¹ ï¼šå‡†ç¡®ç‡ä¸€ç›´å¾˜å¾Šåœ¨ 25-30% (éšæœºæ°´å¹³)")
    print("="*70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/6] åŠ è½½æ•°æ®...")
    try:
        df = pd.read_csv(TRAIN_DATA_PATH)
        print(f"  åŸå§‹æ•°æ®: {len(df)} æ¡")
    except FileNotFoundError:
        print(f"  âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {TRAIN_DATA_PATH}")
        return
    
    # æ ‡ç­¾æ˜ å°„
    label_map = {'Irrelevant': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3}
    if 'attitude_encoded' not in df.columns:
        df['attitude_encoded'] = df['attitude'].map(label_map)
    
    # ä»æ¯ä¸ªç±»åˆ«å‡åŒ€é‡‡æ ·ï¼Œç¡®ä¿ç±»åˆ«å¹³è¡¡
    print(f"\n  ä»æ¯ä¸ªç±»åˆ«é‡‡æ · {TINY_DATASET_SIZE // 4} æ¡...")
    tiny_df_list = []
    for label in range(4):
        label_df = df[df['attitude_encoded'] == label]
        sampled = label_df.sample(n=min(TINY_DATASET_SIZE // 4, len(label_df)), random_state=42)
        tiny_df_list.append(sampled)
    
    tiny_df = pd.concat(tiny_df_list).reset_index(drop=True)
    print(f"  é‡‡æ ·åæ•°æ®: {len(tiny_df)} æ¡")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {tiny_df['attitude_encoded'].value_counts().sort_index().to_dict()}")
    
    # 2. åŠ è½½ tokenizer
    print("\n[2/6] åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)
    print(f"  âœ“ ä»æœ¬åœ°åŠ è½½: {MODEL_NAME}")
    
    # 3. åˆ›å»ºæ•°æ®é›†ï¼ˆè®­ç»ƒé›†=æµ‹è¯•é›†ï¼Œæµ‹è¯•è¿‡æ‹Ÿåˆèƒ½åŠ›ï¼‰
    print("\n[3/6] åˆ›å»ºæ•°æ®é›†...")
    dataset = SentimentDataset(tiny_df, tokenizer, max_length=MAX_LENGTH)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    print(f"  æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"  æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\n[4/6] åˆ›å»ºæ¨¡å‹...")
    model = BertSentimentClassifier(
        model_name=MODEL_NAME,
        num_classes=NUM_CLASSES,
        dropout=DROPOUT,
        freeze_strategy='none'  # å®Œå…¨è§£å†»ï¼Œç¡®ä¿èƒ½å­¦ä¹ 
    ).to(DEVICE)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
    
    # 5. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    print("\n[5/6] è®¾ç½®ä¼˜åŒ–å™¨...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()
    
    # 6. è®­ç»ƒ
    print("\n[6/6] å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    best_accuracy = 0
    overfitting_achieved = False
    
    for epoch in range(NUM_EPOCHS):
        train_loss, train_acc = train_epoch(model, dataloader, optimizer, criterion, DEVICE)
        
        # æ¯ä¸ª epoch éƒ½è¯„ä¼°åŒä¸€ä¸ªæ•°æ®é›†ï¼ˆæµ‹è¯•è¿‡æ‹Ÿåˆï¼‰
        eval_loss, eval_acc = evaluate(model, dataloader, criterion, DEVICE)
        
        # è®°å½•æœ€ä½³å‡†ç¡®ç‡
        if train_acc > best_accuracy:
            best_accuracy = train_acc
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒå‡†ç¡®ç‡ > 90%ï¼‰
        if train_acc >= 90 and not overfitting_achieved:
            overfitting_achieved = True
            print(f"\n{'='*70}")
            print(f"ğŸ‰ è¿‡æ‹Ÿåˆè¾¾æˆï¼Epoch {epoch+1}: è®­ç»ƒå‡†ç¡®ç‡ {train_acc:.2f}%")
            print(f"{'='*70}\n")
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 5 == 0 or epoch < 5 or overfitting_achieved:
            print(f"Epoch {epoch+1:3d}/{NUM_EPOCHS} | "
                  f"Loss: {train_loss:.4f} | "
                  f"Train Acc: {train_acc:6.2f}% | "
                  f"Best: {best_accuracy:6.2f}%")
        
        # å¦‚æœå·²ç»è¾¾åˆ° 95%+ å‡†ç¡®ç‡ï¼Œå¯ä»¥æå‰åœæ­¢
        if train_acc >= 95:
            print(f"\nè®­ç»ƒå‡†ç¡®ç‡è¾¾åˆ° {train_acc:.2f}%ï¼Œæå‰åœæ­¢è®­ç»ƒã€‚")
            break
    
    # 7. æ€»ç»“
    print("\n" + "="*70)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*70)
    print(f"\næœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {best_accuracy:.2f}%")
    
    if best_accuracy >= 90:
        print("\nâœ… æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å…·å¤‡å­¦ä¹ èƒ½åŠ›")
        print("   â†’ æ¨¡å‹æ¶æ„æ­£å¸¸")
        print("   â†’ å¯ä»¥è®°ä½è®­ç»ƒæ•°æ®")
        print("   â†’ å¤§æ•°æ®é›†è®­ç»ƒå¤±è´¥å¯èƒ½æ˜¯:")
        print("      - å­¦ä¹ ç‡ä¸åˆé€‚")
        print("      - æ•°æ®è´¨é‡é—®é¢˜")
        print("      - éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°")
        print("      - æ­£åˆ™åŒ–è¿‡å¼ºï¼ˆdropout/weight decayï¼‰")
    elif best_accuracy >= 60:
        print("\nâš ï¸ éƒ¨åˆ†é€šè¿‡ï¼šæ¨¡å‹æœ‰ä¸€å®šå­¦ä¹ èƒ½åŠ›ï¼Œä½†ä¸å¤Ÿå¼º")
        print("   â†’ å¯èƒ½é—®é¢˜:")
        print("      - å­¦ä¹ ç‡åä½")
        print("      - éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°")
        print("      - Dropout è¿‡é«˜")
        print("      - æ¨¡å‹éƒ¨åˆ†å‚æ•°è¢«å†»ç»“")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼æ¨¡å‹æ— æ³•å­¦ä¹ ")
        print("   â†’ å¯èƒ½é—®é¢˜:")
        print("      - æ¨¡å‹æ¶æ„æœ‰ bug")
        print("      - å‰å‘ä¼ æ’­å®ç°é”™è¯¯")
        print("      - æ¢¯åº¦æ— æ³•åå‘ä¼ æ’­")
        print("      - æ•°æ®é¢„å¤„ç†æœ‰é—®é¢˜")
        print("      - æ ‡ç­¾ç¼–ç é”™è¯¯")
    
    print("="*70)
    
    # 8. è¯¦ç»†è¯Šæ–­ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    if best_accuracy < 60:
        print("\n" + "="*70)
        print("è¯¦ç»†è¯Šæ–­")
        print("="*70)
        
        # æ£€æŸ¥æ¨¡å‹è¾“å‡º
        model.eval()
        with torch.no_grad():
            sample_batch = next(iter(dataloader))
            input_ids = sample_batch['input_ids'].to(DEVICE)
            attention_mask = sample_batch['attention_mask'].to(DEVICE)
            labels = sample_batch['labels'].to(DEVICE)
            
            outputs = model(input_ids, attention_mask)
            
            print(f"\næ ·æœ¬æ‰¹æ¬¡åˆ†æ:")
            print(f"  è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            print(f"  è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
            print(f"  è¾“å‡ºèŒƒå›´: [{outputs.min().item():.4f}, {outputs.max().item():.4f}]")
            print(f"  è¾“å‡ºå‡å€¼: {outputs.mean().item():.4f}")
            print(f"  è¾“å‡ºæ ‡å‡†å·®: {outputs.std().item():.4f}")
            
            # æ£€æŸ¥é¢„æµ‹åˆ†å¸ƒ
            _, predicted = torch.max(outputs, 1)
            print(f"\n  çœŸå®æ ‡ç­¾: {labels.cpu().numpy()}")
            print(f"  é¢„æµ‹æ ‡ç­¾: {predicted.cpu().numpy()}")
            print(f"  é¢„æµ‹åˆ†å¸ƒ: {np.bincount(predicted.cpu().numpy(), minlength=4)}")
        
        # æ£€æŸ¥æ¢¯åº¦
        print(f"\næ¢¯åº¦æ£€æŸ¥:")
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                print(f"  {name}: grad_norm={grad_norm:.6f}")
                if grad_norm < 1e-7:
                    print(f"    âš ï¸ æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½æ— æ³•æ›´æ–°")
            else:
                print(f"  {name}: æ— æ¢¯åº¦ (å¯èƒ½è¢«å†»ç»“)")
        
        print("="*70)

if __name__ == '__main__':
    main()
