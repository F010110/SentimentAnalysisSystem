"""
è¿‡æ‹Ÿåˆæµ‹è¯•è„šæœ¬ v2 - ä¿®å¤ç‰ˆæœ¬
ä¿®å¤é—®é¢˜ï¼š
1. é™ä½å­¦ä¹ ç‡ï¼ˆ5e-5 â†’ 1e-5ï¼‰é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
2. ä½¿ç”¨æ¢¯åº¦è£å‰ª
3. åªè§£å†»æœ€å2å±‚BERT + åˆ†ç±»å¤´ï¼ˆå‡å°‘è®­ç»ƒéš¾åº¦ï¼‰
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer
import pandas as pd
import numpy as np
import sys
import os

sys.path.append(os.path.dirname(__file__))
from BERT_model_final import BertSentimentClassifier

# ==================== é…ç½®å‚æ•° ====================
MODEL_NAME = 'models/bert-base-uncased'
TRAIN_DATA_PATH = 'dataset/twitter_training_cleaned.csv'

# å°æ•°æ®é›†é…ç½® - ä¿®å¤ç‰ˆ
TINY_DATASET_SIZE = 100
BATCH_SIZE = 8
LEARNING_RATE = 1e-5      # âœ“ é™ä½å­¦ä¹ ç‡ï¼ˆåŸæ¥5e-5å¤ªé«˜ï¼‰
NUM_EPOCHS = 100          # âœ“ å¢åŠ è½®æ•°
MAX_LENGTH = 128
MAX_GRAD_NORM = 1.0       # âœ“ æ·»åŠ æ¢¯åº¦è£å‰ª

# å…¶ä»–å‚æ•°
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
NUM_CLASSES = 4
DROPOUT = 0.1
FREEZE_STRATEGY = 'half'  # âœ“ åªè®­ç»ƒæœ€åå‡ å±‚ï¼ˆæ›´å®¹æ˜“æ”¶æ•›ï¼‰

# ==================== æ•°æ®é›† ====================
class SentimentDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.texts = dataframe['cleaned_text'].values
        self.labels = dataframe['attitude_encoded'].values
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_epoch(model, dataloader, optimizer, criterion, device, max_grad_norm):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        optimizer.step()
        
        total_loss += loss.item()
        
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy

def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy

# ==================== ä¸»å‡½æ•° ====================
def main():
    print("="*70)
    print("BERT è¿‡æ‹Ÿåˆæµ‹è¯• v2 - ä¿®å¤ç‰ˆæœ¬")
    print("="*70)
    print(f"\nä¿®å¤å†…å®¹:")
    print(f"  âœ“ å­¦ä¹ ç‡: 5e-5 â†’ {LEARNING_RATE}")
    print(f"  âœ“ å†»ç»“ç­–ç•¥: none â†’ {FREEZE_STRATEGY} (åªè®­ç»ƒååŠéƒ¨åˆ†)")
    print(f"  âœ“ æ·»åŠ æ¢¯åº¦è£å‰ª: max_norm={MAX_GRAD_NORM}")
    print(f"  âœ“ å¢åŠ è®­ç»ƒè½®æ•°: 50 â†’ {NUM_EPOCHS}")
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  æ•°æ®é›†å¤§å°: {TINY_DATASET_SIZE} æ¡")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  è®¾å¤‡: {DEVICE}")
    print(f"\né¢„æœŸç»“æœ:")
    print(f"  âœ“ 20-30 è½®å†…è¾¾åˆ° 90%+ å‡†ç¡®ç‡")
    print("="*70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/6] åŠ è½½æ•°æ®...")
    df = pd.read_csv(TRAIN_DATA_PATH)
    print(f"  åŸå§‹æ•°æ®: {len(df)} æ¡")
    
    # æ ‡ç­¾æ˜ å°„
    label_map = {'Irrelevant': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3}
    if 'attitude_encoded' not in df.columns:
        df['attitude_encoded'] = df['attitude'].map(label_map)
    
    # ä»æ¯ä¸ªç±»åˆ«é‡‡æ ·
    tiny_df_list = []
    for label in range(4):
        label_df = df[df['attitude_encoded'] == label]
        sampled = label_df.sample(n=min(TINY_DATASET_SIZE // 4, len(label_df)), random_state=42)
        tiny_df_list.append(sampled)
    
    tiny_df = pd.concat(tiny_df_list).reset_index(drop=True)
    print(f"  é‡‡æ ·æ•°æ®: {len(tiny_df)} æ¡")
    
    # 2. åŠ è½½ tokenizer
    print("\n[2/6] åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)
    print(f"  âœ“ åŠ è½½å®Œæˆ")
    
    # 3. åˆ›å»ºæ•°æ®é›†
    print("\n[3/6] åˆ›å»ºæ•°æ®é›†...")
    dataset = SentimentDataset(tiny_df, tokenizer, max_length=MAX_LENGTH)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    print(f"  æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\n[4/6] åˆ›å»ºæ¨¡å‹...")
    model = BertSentimentClassifier(
        model_name=MODEL_NAME,
        num_classes=NUM_CLASSES,
        dropout=DROPOUT,
        freeze_strategy=FREEZE_STRATEGY
    ).to(DEVICE)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
    
    # 5. è®¾ç½®ä¼˜åŒ–å™¨
    print("\n[5/6] è®¾ç½®ä¼˜åŒ–å™¨...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()
    
    # 6. è®­ç»ƒ
    print("\n[6/6] å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    best_accuracy = 0
    overfitting_achieved = False
    patience = 0
    max_patience = 20  # å¦‚æœ20è½®æ²¡æå‡å°±åœæ­¢
    
    for epoch in range(NUM_EPOCHS):
        train_loss, train_acc = train_epoch(model, dataloader, optimizer, criterion, DEVICE, MAX_GRAD_NORM)
        
        if train_acc > best_accuracy:
            best_accuracy = train_acc
            patience = 0
        else:
            patience += 1
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
        if train_acc >= 90 and not overfitting_achieved:
            overfitting_achieved = True
            print(f"\n{'='*70}")
            print(f"ğŸ‰ è¿‡æ‹Ÿåˆè¾¾æˆï¼Epoch {epoch+1}: è®­ç»ƒå‡†ç¡®ç‡ {train_acc:.2f}%")
            print(f"{'='*70}\n")
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 5 == 0 or epoch < 10 or overfitting_achieved:
            print(f"Epoch {epoch+1:3d}/{NUM_EPOCHS} | "
                  f"Loss: {train_loss:.4f} | "
                  f"Train Acc: {train_acc:6.2f}% | "
                  f"Best: {best_accuracy:6.2f}%")
        
        # æå‰åœæ­¢
        if train_acc >= 95:
            print(f"\nâœ“ è®­ç»ƒå‡†ç¡®ç‡è¾¾åˆ° {train_acc:.2f}%ï¼Œæå‰åœæ­¢ã€‚")
            break
        
        if patience >= max_patience:
            print(f"\nâš ï¸ {max_patience} è½®æ— æå‡ï¼Œæå‰åœæ­¢ã€‚")
            break
    
    # 7. æ€»ç»“
    print("\n" + "="*70)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*70)
    print(f"\næœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {best_accuracy:.2f}%")
    
    if best_accuracy >= 90:
        print("\nâœ… æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å…·å¤‡å­¦ä¹ èƒ½åŠ›")
        print("   â†’ æ¨¡å‹æ¶æ„æ­£å¸¸")
        print("   â†’ å¯ä»¥è®°ä½è®­ç»ƒæ•°æ®")
        print("   â†’ å¤§æ•°æ®é›†é—®é¢˜å¯èƒ½æ˜¯:")
        print("      - å­¦ä¹ ç‡éœ€è¦è°ƒæ•´")
        print("      - éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°")
        print("      - æ•°æ®è´¨é‡/æ ‡ç­¾é—®é¢˜")
    elif best_accuracy >= 70:
        print("\nâš ï¸ éƒ¨åˆ†é€šè¿‡ï¼šæ¨¡å‹æœ‰å­¦ä¹ èƒ½åŠ›ä½†è¾ƒå¼±")
        print("   â†’ å»ºè®®:")
        print("      - è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡")
        print("      - å»¶é•¿è®­ç»ƒæ—¶é—´")
        print("      - ä½¿ç”¨æ›´æ¿€è¿›çš„è§£å†»ç­–ç•¥")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼æ¨¡å‹å­¦ä¹ èƒ½åŠ›ä¸è¶³")
        print(f"   â†’ å½“å‰å‡†ç¡®ç‡ {best_accuracy:.2f}% æ¥è¿‘éšæœºæ°´å¹³ (25%)")
        print("   â†’ å¯èƒ½é—®é¢˜:")
        print("      - å­¦ä¹ ç‡ä»ç„¶è¿‡é«˜æˆ–è¿‡ä½")
        print("      - æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸")
        print("      - æ¨¡å‹æ¶æ„é—®é¢˜")
    
    print("="*70)

if __name__ == '__main__':
    main()
