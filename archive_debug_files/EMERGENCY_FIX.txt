【紧急诊断】模型没有学习的问题
=====================================================

观察到的症状:
- Loss 几乎不变: 1.7680 → 1.7452 (只下降0.02)
- 准确率停滞: 25-31% (随机基线)
- 批次准确率极低且不稳定 (0-56%)

根本原因:
⚠️ 学习率 1e-4 对于完全解冻的 BERT 太高了！

完全解冻 BERT (110M 参数) 需要**非常小的学习率**，否则:
- 预训练权重被破坏
- 梯度震荡/爆炸
- 无法收敛

【立即停止当前训练】

在云服务器上按 Ctrl+C 停止当前训练

【修复方案】

方案 A: 降低学习率 (最简单)
-----------------------------------
已修复 BERT_main_aggressive.py:
- LEARNING_RATE: 1e-4 → 2e-5 ✅
- WARMUP_STEPS: 1000 → 2000 ✅
- NUM_EPOCHS: 15 → 20 ✅

重新运行:
$ python BERT_main_aggressive.py

预期: Loss 应该稳定下降，准确率逐步上升


方案 B: 分层学习率 (推荐，更稳定)
-----------------------------------
使用新脚本 BERT_train_layerwise_lr.py:

策略:
- 分类头: 1e-4 (最高，新初始化的层)
- BERT 上层 (9-12): 2e-5
- BERT 中层 (5-8): ~1.5e-5 (衰减)
- BERT 下层 (0-4): ~1e-5 (衰减)
- Embedding: 2e-6 (最低，最不想改变)

运行:
$ python BERT_train_layerwise_lr.py

优势:
✓ 更稳定的训练
✓ 保护预训练权重
✓ 快速适应分类头


方案 C: 渐进式解冻 (保守)
-----------------------------------
如果方案A/B还是不行，逐步解冻:

第1阶段: 只训练分类头
- FREEZE_STRATEGY = 'full' (冻结全部BERT)
- LEARNING_RATE = 1e-3
- EPOCHS = 5

第2阶段: 解冻后3层
- FREEZE_STRATEGY = 'half' 
- LEARNING_RATE = 5e-5
- EPOCHS = 10

第3阶段: 完全解冻
- FREEZE_STRATEGY = 'none'
- LEARNING_RATE = 2e-5
- EPOCHS = 10


【为什么 1e-4 太高】

BERT 经验法则:
- 冻结大部分层 (只训练分类头): 1e-3 ~ 1e-4
- 解冻少数层 (微调后几层): 5e-5 ~ 1e-4
- 完全解冻 (微调全部): 2e-5 ~ 5e-5  ← 你的情况
- 继续预训练: 1e-5 ~ 2e-5

使用 1e-4 完全解冻 = 学习率太大 5 倍！

【监控指标】

重新训练时，成功的信号:
✓ Loss 每个 epoch 下降 >= 0.05
✓ 准确率每个 epoch 上升 >= 2%
✓ Batch 准确率逐渐提高
✓ 第5个 epoch 准确率应该 > 50%

失败的信号:
✗ Loss 停滞或震荡
✗ 准确率不变或下降
✗ Batch 准确率随机波动

【立即行动】

1. 停止当前训练 (Ctrl+C)

2. 选择方案:
   
   简单快速: 
   $ python BERT_main_aggressive.py  # 已修复
   
   推荐稳定:
   $ python BERT_train_layerwise_lr.py  # 分层LR
   
3. 监控前 5 个 epoch:
   - Loss 应该从 1.4 下降到 < 1.0
   - 准确率应该从 25% 上升到 > 50%

4. 如果还是不行:
   - 使用方案 C (渐进式解冻)
   - 或考虑先用 processed_text 看看是否是数据问题

【预期结果】

修复后:
Epoch 1: Loss 1.35, Acc 35%
Epoch 2: Loss 1.15, Acc 45%
Epoch 3: Loss 0.95, Acc 55%
Epoch 5: Loss 0.75, Acc 65%
Epoch 10: Loss 0.45, Acc 75-80%

立即停止当前训练，使用修复后的配置重新开始！
