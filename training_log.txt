背景：
    在人工智能导论课发布作业之前，我已经做了一个半成品的情感分析系统，基于RNN（实际上是GRU）。
    为什么说是半成品呢？因为GRU的准确率只有92.9%，而基准模型中的SVM、随机森林、
    XGBoost、KNN的准确率都比这高，其中SVM的准确率高达96.2% 。
    现在我先要对GRU调参，或者找出基准模型准确率普遍高的原因。





问题1：怎么改进GRU？

AI建议：
    查看混淆矩阵：RNN在哪几类上表现差？
    检查词表覆盖率：有多少测试集词被映射为<UNK>？
    对比特征：用TF-IDF特征训练一个简单的NN，看是否能接近SVM效果
    学习曲线：RNN是否过拟合/欠拟合？
    
    建议先调整清洗策略和使用预训练词向量，这两个改动最容易实施且通常效果最明显。

我现在尝试学VScode远程连接实例……

和deepseek斗智斗勇半天终于学会了。
deepseek以为我很聪明，没要考虑到我在尝试远程连接之前没有启动实例的可能…………
而我之所以在尝试远程连接之前没有启动实例，仅仅是为了节省几分钱。

我微调了数据清洗策略（再negation_words中补充程度副词，并确保单字母词i、a、u不被移除），
又大幅调整模型参数：
    EMBED_DIM = 300        # 增加嵌入维度（原128）
    HIDDEN_DIM = 256       # 增加隐藏层维度（原64）
    N_LAYERS = 3           # 增加层数（原2）
    DROPOUT = 0.5          # 增加dropout（原0.3）
    BATCH_SIZE = 64        # 增加批大小（原32）
    LEARNING_RATE = 0.0005 # 降低学习率（原0.001）

现在GRU的准确率达到95.5%，但离SVM还有差距。

AI建议：
    先试试类权重调整（最快，可能直接提升0.2-0.3%）
    修改优化器为AdamW + 微调学习率
    改进GRU的forward方法（加入池化）
    添加简单的注意力机制
    使用加权采样

我先计算类别权重以解决解决潜在的不平衡问题，修改criterion。
然后把optimizer从Adam改为AdamW，并减小weight_decay。
然后改进GRU隐藏状态的使用，引入了平均池化和最大池化，再把结果concat。
    这样得再把self.fc的维度改一下。这Deepseek竟然不提醒我。还好我不傻。我让copilot给我改。
    还得是copilot中的claude sonnet靠谱。一下就改对。
然后我再修改dropout策略…………我怎么没看懂deepseek让我干什么…………算了，不改了。

跑一个看看…………这次Train Acc上升的很快啊！

但是测试集准确率还是只有95.7%，而Train Acc达到97.95%，看起来是有点过拟合。
再问问AI…………

AI建议：
    1. 降低模型复杂度
    2. 添加早停机制
    3. 使用标签平滑

先试试 降低模型复杂度：
    EMBED_DIM = 200        # 减少嵌入维度（原300）
    HIDDEN_DIM = 128       # 减少隐藏层维度（原256）
    N_LAYERS = 2           # 减少层数（原3）
    DROPOUT = 0.6          # 增加dropout（原0.5）

现在准确率达到96.0%，我再减少一些epochs。

还是96.0%，我再随便调调参数。

把N_LAYERS改到4，准确率最高的一次达到96.8%！
但这也只是昙花一现。
同样的参数跑出来的模型性能也有差距，应该是因为dropout是随机的。

调试了几个小时，再也没有调出比96.8更高的了。显然这个远远没有达到我的预期。应该是我太菜了。
就这样吧。
我接下来试试BERT。






问题2：如何搭建BERT？

由于本人只是初学者，完全没有独立搭建BERT的能力，我直接让copilot（claude haiku）给我写了一个。
六百多行【害怕】【害怕】…………
我先试着跑通…………

？？？？？？准确率？？？？？？

《 验证集最终准确率:0.2860 》
《 验证集最终损失:1.3827 》

逆如天。猎奇结果。





问题3：如何拯救BERT？

跟copilot斗智斗勇几个小时，不见成效。
冻结层数、学习率这两个参数来回调，没有效果。甚至越调准确率越低。

一怒之下直接告诉copilot这个BERT模型太差，再不行就放弃模型，
并指出我的SVM和GRU表现得多么出色。

效果立竿见影。第一个epoch就有50%准确率。第二个epoch准确率达到80%。

根据copilot，成功原因如下：
    参数            失败版本	成功版本	    影响

    Batch Size	    16	        8	          更小
    梯度累积	     4	         1 (无)	     更频繁更新
    有效Batch	    16×4=64	    8	        8倍差距！
    学习率	        2e-5	    1e-5	    匹配小batch
    Dropout	        0.3	        0.1	        更少正则化

核心问题：
    有效batch size = 64 太大 → 梯度更新太慢
    学习率2e-5对于大batch偏低 → 权重更新幅度小
    Dropout 0.3太高 → 训练时丢弃太多信息

也就是说根本在于batch_size过大。我调参数时从没想过batch_size有什么大的作用。

现在准确率高达98% ！！！

