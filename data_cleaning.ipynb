{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05b4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 下载必要的NLTK数据（第一次运行需要）\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 移除URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 移除用户提及 @username\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 移除话题标签 #hashtag (但保留文本部分)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # 移除HTML标签\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 移除标点符号\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 移除数字\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 移除多余空白字符\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8dbe8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_tweet_cleaning(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # 基础清洗\n",
    "    text = clean_tweet_text(text)\n",
    "    \n",
    "    # 分词：优先使用 NLTK 的 word_tokenize，缺少资源时回退到正则分词\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except LookupError:\n",
    "        tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "    # 移除停用词：优先使用 NLTK 停用词，缺少资源时使用内置小型停用词集合\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        stop_words = {\n",
    "            'the','a','an','and','or','but','if','in','on','for','with',\n",
    "            'is','it','this','that','to','of','at','from','by','as','are',\n",
    "            'was','were','be','been','has','have','had','not','no','so',\n",
    "            'too','very'\n",
    "        }\n",
    "\n",
    "    # 保留否定词（重要：先保留否定词，再去除其他停用词）\n",
    "    negation_words = ['not', 'no', 'never', 'nothing', 'nowhere', 'neither', 'nor', \n",
    "                      'very', 'so', 'too', 'extremely', 'absolutely']\n",
    "    stop_words = set(w for w in stop_words if w not in negation_words)\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 移除短词（长度小于2的单词）\n",
    "    important_single_letters = {'i', 'a', 'u'}  # I, a, you的缩写\n",
    "    tokens = [word for word in tokens if len(word) > 1 or word in important_single_letters]\n",
    "    \n",
    "    # 重新组合为文本\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e70b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis_and_special_chars(text):\n",
    "    \"\"\"\n",
    "    处理表情符号：可以选择移除或替换为文字描述\n",
    "    \"\"\"\n",
    "    # 方法1: 移除所有表情符号\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # 表情符号\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # 符号和象形文字\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # 交通和地图符号\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # 国旗\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # 方法2: 替换常见表情符号为文字描述（可选）\n",
    "    emoji_to_text = {\n",
    "        ':)': 'smile',\n",
    "        ':(': 'sad',\n",
    "        ':D': 'laugh',\n",
    "        ';)': 'wink'\n",
    "        # 可以添加更多映射\n",
    "    }\n",
    "    \n",
    "    for emoji, desc in emoji_to_text.items():\n",
    "        text = text.replace(emoji, desc)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9267da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text_cleaning(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    完整的文本清洗流程\n",
    "    \"\"\"\n",
    "    print(\"开始文本清洗...\")\n",
    "    \n",
    "    # 复制数据以避免修改原数据\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # 1. 处理缺失值\n",
    "    df_cleaned[text_column] = df_cleaned[text_column].fillna('')\n",
    "    \n",
    "    # 2. 基础文本清洗\n",
    "    df_cleaned['cleaned_text'] = df_cleaned[text_column].apply(clean_tweet_text)\n",
    "    \n",
    "    # 3. 处理表情符号\n",
    "    df_cleaned['cleaned_text'] = df_cleaned['cleaned_text'].apply(handle_emojis_and_special_chars)\n",
    "    \n",
    "    # 4. 高级清洗（分词、去停用词等）\n",
    "    df_cleaned['processed_text'] = df_cleaned['cleaned_text'].apply(advanced_tweet_cleaning)\n",
    "    \n",
    "    # 5. 移除空文本\n",
    "    original_count = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned[df_cleaned['processed_text'].str.len() > 0]\n",
    "    cleaned_count = len(df_cleaned)\n",
    "    \n",
    "    print(f\"清洗完成！原始数据: {original_count} 条，清洗后: {cleaned_count} 条\")\n",
    "    print(f\"移除了 {original_count - cleaned_count} 条空文本\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8827b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据列: ['ID', 'area', 'attitude', 'text']\n",
      "原始数据示例:\n",
      "     ID         area  attitude  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "                                                text  \n",
      "0  im getting on borderlands and i will murder yo...  \n",
      "1  I am coming to the borders and I will kill you...  \n",
      "2  im getting on borderlands and i will kill you ...  \n",
      "3  im coming on borderlands and i will murder you...  \n",
      "4  im getting on borderlands 2 and i will murder ...  \n",
      "使用检测到的文本列: text\n",
      "开始文本清洗...\n",
      "清洗完成！原始数据: 74682 条，清洗后: 72142 条\n",
      "移除了 2540 条空文本\n",
      "\n",
      "清洗前后对比:\n",
      "原始文本: im getting on borderlands and i will murder you all ,\n",
      "清洗后文本: im getting borderlands murder\n",
      "\n",
      "清洗后的文本统计:\n",
      "平均长度: 72.15 字符\n",
      "最短文本: 1 字符\n",
      "最长文本: 839 字符\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_path = os.path.join('dataset', 'twitter_training.csv')\n",
    "val_path = os.path.join('dataset', 'twitter_validation.csv')\n",
    "train_cleaned_path = os.path.join('dataset', 'twitter_training_cleaned.csv')\n",
    "val_cleaned_path = os.path.join('dataset', 'twitter_validation_cleaned.csv')\n",
    "\n",
    "df = pd.read_csv(train_path, names=['ID','area','attitude','text'], header=None)\n",
    "\n",
    "# 打印列名，帮助定位文本列\n",
    "print('数据列:', df.columns.tolist())\n",
    "print('原始数据示例:')\n",
    "print(df.head())\n",
    "\n",
    "# 自动检测合适的文本列\n",
    "possible_text_cols = ['text','tweet','content','message','comment','body','sentiment_text']\n",
    "text_cols = [c for c in df.columns if c.lower() in possible_text_cols]\n",
    "if len(text_cols) == 0:\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if len(obj_cols) == 0:\n",
    "        raise ValueError('未在数据中找到文本列（object dtype）。请检查CSV列名。')\n",
    "    chosen = obj_cols[0]\n",
    "    print(f\"没有找到常见的文本列，使用第一列: {chosen}\")\n",
    "else:\n",
    "    chosen = text_cols[0]\n",
    "    print(f\"使用检测到的文本列: {chosen}\")\n",
    "\n",
    "# 应用清洗流程（使用检测到的列名）\n",
    "df_cleaned = complete_text_cleaning(df, text_column=chosen)\n",
    "\n",
    "# 查看清洗前后的对比（谨慎处理索引越界）\n",
    "print('\\n清洗前后对比:')\n",
    "sample_idx = 0\n",
    "if sample_idx >= len(df):\n",
    "    sample_idx = df.index[0]\n",
    "orig_text = df.iloc[sample_idx][chosen] if chosen in df.columns else ''\n",
    "print('原始文本:', orig_text)\n",
    "print('清洗后文本:', df_cleaned.iloc[sample_idx]['processed_text'] if sample_idx in df_cleaned.index else df_cleaned['processed_text'].iloc[0])\n",
    "\n",
    "# 检查清洗效果\n",
    "print('\\n清洗后的文本统计:')\n",
    "print(f\"平均长度: {df_cleaned['processed_text'].str.len().mean():.2f} 字符\")\n",
    "print(f\"最短文本: {df_cleaned['processed_text'].str.len().min()} 字符\")\n",
    "print(f\"最长文本: {df_cleaned['processed_text'].str.len().max()} 字符\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da6dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(train_cleaned_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d72d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证数据列: ['ID', 'area', 'attitude', 'text']\n",
      "     ID       area    attitude  \\\n",
      "0  3364   Facebook  Irrelevant   \n",
      "1   352     Amazon     Neutral   \n",
      "2  8312  Microsoft    Negative   \n",
      "3  4371      CS-GO    Negative   \n",
      "4  4433     Google     Neutral   \n",
      "\n",
      "                                                text  \n",
      "0  I mentioned on Facebook that I was struggling ...  \n",
      "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
      "2  @Microsoft Why do I pay for WORD when it funct...  \n",
      "3  CSGO matchmaking is so full of closet hacking,...  \n",
      "4  Now the President is slapping Americans in the...  \n",
      "使用检测到的文本列: text\n",
      "开始文本清洗...\n",
      "清洗完成！原始数据: 1000 条，清洗后: 1000 条\n",
      "移除了 0 条空文本\n",
      "已保存: twitter_validation_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# 读取并清洗验证集\n",
    "df_val = pd.read_csv(val_path, names=['ID','area','attitude','text'], header=None)\n",
    "\n",
    "# 打印列名与示例以便确认\n",
    "print('验证数据列:', df_val.columns.tolist())\n",
    "print(df_val.head())\n",
    "\n",
    "# 自动检测文本列（与训练集相同的策略）\n",
    "possible_text_cols = ['text','tweet','content','message','comment','body','sentiment_text']\n",
    "text_cols = [c for c in df_val.columns if c.lower() in possible_text_cols]\n",
    "if len(text_cols) == 0:\n",
    "    obj_cols = df_val.select_dtypes(include=['object']).columns.tolist()\n",
    "    if len(obj_cols) == 0:\n",
    "        raise ValueError('未在验证数据中找到文本列（object dtype）。请检查CSV列名。')\n",
    "    chosen_val = obj_cols[0]\n",
    "    print(f\"没有找到常见的文本列，使用第一列: {chosen_val}\")\n",
    "else:\n",
    "    chosen_val = text_cols[0]\n",
    "    print(f\"使用检测到的文本列: {chosen_val}\")\n",
    "\n",
    "# 对验证集应用相同的清洗流程\n",
    "df_val_cleaned = complete_text_cleaning(df_val, text_column=chosen_val)\n",
    "\n",
    "# 保存清洗后的验证集\n",
    "df_val_cleaned.to_csv(val_cleaned_path, index=False)\n",
    "print('已保存: twitter_validation_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55b764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
